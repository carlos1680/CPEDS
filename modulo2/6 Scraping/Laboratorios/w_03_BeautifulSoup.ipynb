{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd062b65a3e32fd75c6613f438e3e64606454bf9518010d2b50fe61db1c37149f34",
   "display_name": "Python 3.7.9 64-bit ('Scrapping-CfF0pIyJ': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "w_03 | BeautifulSoup\n",
    "===="
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "BeautifulSoup es, junto con Scrapy, el paquete más usado para scrapping web. Su uso básico tiene una lógica muy simple que consiste en proveerle el código fuente de una página (el HTML) para que construya un \"árbol\" con sus elementos en donde podemos buscar lo que necesitamos.\n",
    "\n",
    "Busquemos todas las noticias de la portada de Montevideo Portal, para lo que hay que inspeccionar la fuente.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1ypYmHKOQX281hqE5aLv0J39s0-oP56A4)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{   'Argentina suspendió por 30 días las exportaciones de carne para contener el precio interno': 'https://www.montevideo.com.uy/Noticias/Argentina-suspendio-por-30-dias-las-exportaciones-de-carne-para-contener-el-precio-interno-uc786772',\n    'Biden expresa su apoyo a un alto el fuego durante una llamada con Netanyahu': 'https://www.montevideo.com.uy/Noticias/Biden-expresa-su-apoyo-a-un-alto-el-fuego-durante-una-llamada-con-Netanyahu-uc786777',\n    'Bomberos extinguió incendio en el histórico teatro 28 de Febrero de Mercedes, en Soriano': 'https://www.montevideo.com.uy/Noticias/Bomberos-extinguio-incendio-en-el-historico-teatro-28-de-Febrero-de-Mercedes-en-Soriano-uc786770',\n    'Duque ordena desplegar a la fuerza pública para desbloquear vías en Colombia': 'https://www.montevideo.com.uy/Noticias/Duque-ordena-desplegar-a-la-fuerza-publica-para-desbloquear-vias-en-Colombia-uc786778',\n    'Hubo 62 fallecimientos y se registraron 2.399 casos nuevos de covid en 11.184 análisis': 'https://www.montevideo.com.uy/Noticias/Hubo-62-fallecimientos-y-se-registraron-2-399-casos-nuevos-de-covid-en-11-184-analisis-uc786766',\n    'La carta de Mujica, Lula, Rousseff y Evo Morales ante escalada de violencia en Colombia': 'https://www.montevideo.com.uy/Noticias/La-carta-de-Mujica-Lula-Rousseff-y-Evo-Morales-ante-escalada-de-violencia-en-Colombia-uc786776',\n    'Mides y el Plan de Invierno: estudian aplicar una ley votada en el período de José Mujica': 'https://www.montevideo.com.uy/Noticias/Mides-y-el-Plan-de-Invierno-estudian-aplicar-una-ley-votada-en-el-periodo-de-Jose-Mujica-uc786773',\n    'Opinión | Comunicados 4 y 7: el poder y la miseria': 'https://www.montevideo.com.uy/Columnistas/Opinion--Comunicados-4-y-7-el-poder-y-la-miseria-uc786218',\n    'Opinión | Cuidado con las mentiras de los terroristas sobre sus muertos': 'https://www.montevideo.com.uy/Columnistas/Opinion--Cuidado-con-las-mentiras-de-los-terroristas-sobre-sus-muertos-uc786207',\n    'Opinión | Los Andes de Luisa: la última Cuesta': 'https://www.montevideo.com.uy/Columnistas/Opinion--Los-Andes-de-Luisa-la-ultima-Cuesta-uc786683',\n    'Opinión |Borrar fronteras, fortalecer países': 'https://www.montevideo.com.uy/Columnistas/Opinion-Borrar-fronteras-fortalecer-paises-uc786737',\n    'Pequeño incendio de campo en una zona anterior a la playa, cerca de la Plaza Virgilio': 'https://www.montevideo.com.uy/Noticias/Pequeno-incendio-de-campo-en-una-zona-anterior-a-la-playa-cerca-de-la-Plaza-Virgilio-uc786765'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.montevideo.com.uy/categoria/Noticias-310\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content)\n",
    "find = soup.find_all(\"h2\", class_=\"title\")\n",
    "results = {el.contents[0].text: el.contents[0].get(\"href\") for el in find}\n",
    "pprint(results, indent=4)"
   ]
  },
  {
   "source": [
    "Hicimos lo siguiente:\n",
    "\n",
    "1. Un request para obtener el HTML.\n",
    "2. Pasamos el contenido de la respuesta a la clase ``BeautifulSoup`` (se llama igual que el paquete, pero se importa de ``bs4``), lo que devuelve un objeto que podemos procesar fácilmente.\n",
    "3. Usamos el método ``find_all()`` para que encuentre todos los elementos con el tag h2 y la class \"title\", porque eso es lo que figura cuando hacemos inspect en un browser.\n",
    "4. El texto y el link están un nivel más abajo del tag h2, por lo que tenemos que acceder a los ``contents`` (es una lista, por eso accedemos al elemento 0, el único que hay en este caso).\n",
    "5. Armamos una dict comprehension que extrae el texto y el link (href) de cada elemento encontrado.\n",
    "\n",
    "¿Por qué no buscamos directamente los tags a (que son links)? Porque necesitamos alguna forma de \"filtrar\" los resultados, y como ese tag no tiene demasiados detalles, recurrimos a su parent h2. Sino, pasa lo siguiente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<a class=\"icon notificaciones\" href=\"\" onclick=\"return false;\" target=\"_blank\" title=\"Notificaciones\"></a>,\n",
       " <a class=\"later notificationsReminder\" href=\"#\">Recordar más tarde</a>,\n",
       " <a class=\"icon facebook\" href=\"https://www.facebook.com/portalmvd\" target=\"_blank\" title=\"Facebook\"></a>,\n",
       " <a class=\"icon twitter\" href=\"https://twitter.com/portalmvd\" target=\"_blank\" title=\"Twitter\"></a>,\n",
       " <a class=\"icon instagram\" href=\"https://www.instagram.com/portalmvd/\" target=\"_blank\" title=\"Instagram\"></a>,\n",
       " <a class=\"icon youtube\" href=\"https://www.youtube.com/c/MontevideoPortalVideos\" target=\"_blank\" title=\"Youtube\"></a>,\n",
       " <a class=\"icon spotify\" href=\"https://open.spotify.com/show/7qCGYvcXw1WzqRH5I5UZfh\" target=\"_blank\" title=\"Spotify\"></a>,\n",
       " <a class=\"icon rss\" href=\"https://www.montevideo.com.uy/auc.aspx?27383\" target=\"_blank\" title=\"RSS\"></a>,\n",
       " <a class=\"icon itunes\" href=\"https://itunes.apple.com/app/id400466279\" target=\"_blank\" title=\"iTunes\"></a>,\n",
       " <a class=\"icon android\" href=\"https://play.google.com/store/apps/details?id=com.artech.mvdportalmovil3.MontevideoPortalAndroid&amp;feature=search_result\" target=\"_blank\" title=\"Google Play\"></a>]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "soup.find_all(\"a\")[:10]"
   ]
  },
  {
   "source": [
    "Con el método ``find_all()`` y un poco de creatividad se puede hacer prácticamente todo.\n",
    "\n",
    "La idea ahora es extraer avisos de puestos de trabajo de El Gallito. En la URL de búsqueda hay información limitada, por lo que habrá que entrar uno por uno, para lo cual necesitamos extraer las URLs.\n",
    "\n",
    "Vamos a concentranos en los resultados de la primera página.\n",
    "\n",
    "El proceso es igual, inspeccionamos el elemento que nos interesa, ubicamos los tags relevantes y extraemos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "gallito = \"https://trabajo.gallito.com.uy/buscar\"\n",
    "r = requests.get(gallito)\n",
    "soup = BeautifulSoup(r.content)\n",
    "find = soup.find_all(\"a\", class_=\"post-cuadro row smB\")\n",
    "urls = [puesto.get(\"href\") for puesto in find]\n",
    "urls"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/anuncio/administrativo-contable-h4dj9',\n",
       " '/anuncio/fonoaudiologo-a-hnz2e',\n",
       " '/anuncio/vendedor-distribuidor-freelance-n9szv',\n",
       " '/anuncio/ayudante-de-panaderia-avanzado-qm2r9',\n",
       " '/anuncio/limpiador-a-para-panaderia-6arrb',\n",
       " '/anuncio/vendedores-part-time-full-time-1-ezq5b',\n",
       " '/anuncio/vendedores-as-2-patz3',\n",
       " '/anuncio/cuidador-de-las-personas-mayores-pm-en-residenciales-mu98z',\n",
       " '/anuncio/independiente-8t7gq',\n",
       " '/anuncio/responsable-de-administracion-ra5v9',\n",
       " '/anuncio/oficial-electricista-4mv8j',\n",
       " '/anuncio/cobrador-con-moto-para-area-metropolitana-77sg5',\n",
       " '/anuncio/cortador-srb2j',\n",
       " '/anuncio/c-bjb6y',\n",
       " '/anuncio/vendedor-de-calle-con-moto-para-costa-de-oro-vm3mq',\n",
       " '/anuncio/ortodoncista-5gtsm',\n",
       " '/anuncio/agente-de-ventas-online-teletrabajo-6a3re',\n",
       " '/anuncio/peon-v7rss',\n",
       " '/anuncio/acompanante-con-cama-n6jdy',\n",
       " '/anuncio/docente-profesor-9jdm9']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "source": [
    "La tarea ahora es constuir las URLs, hacer el request y luego el scrapping.\n",
    "\n",
    "La ventaja de HTML es que suele estar todo en el mismo lugar. Entonces con ver la estructura en un puesto podemos saber cómo van a funcionar el resto."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Procesando https://trabajo.gallito.com.uy/anuncio/administrativo-contable-h4dj9\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/fonoaudiologo-a-hnz2e\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/vendedor-distribuidor-freelance-n9szv\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/ayudante-de-panaderia-avanzado-qm2r9\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/limpiador-a-para-panaderia-6arrb\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/vendedores-part-time-full-time-1-ezq5b\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/vendedores-as-2-patz3\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/cuidador-de-las-personas-mayores-pm-en-residenciales-mu98z\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/independiente-8t7gq\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/responsable-de-administracion-ra5v9\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/oficial-electricista-4mv8j\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/cobrador-con-moto-para-area-metropolitana-77sg5\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/cortador-srb2j\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/c-bjb6y\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/vendedor-de-calle-con-moto-para-costa-de-oro-vm3mq\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/ortodoncista-5gtsm\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/agente-de-ventas-online-teletrabajo-6a3re\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/peon-v7rss\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/acompanante-con-cama-n6jdy\n",
      "Procesando https://trabajo.gallito.com.uy/anuncio/docente-profesor-9jdm9\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       Descripción              Empresa  Cantidad de ofertas  \\\n",
       "0          ADMINISTRATIVO CONTABLE                       1 ofertas laborales   \n",
       "1                  FONOAUDIÓLOGO/A  Consultorio privado  2 ofertas laborales   \n",
       "2  VENDEDOR DISTRIBUIDOR FREELANCE    Diamante del Este  1 ofertas laborales   \n",
       "3   AYUDANTE DE PANADERÍA AVANZADO              Empresa  2 ofertas laborales   \n",
       "4       LIMPIADOR/A PARA PANADERIA              Empresa  2 ofertas laborales   \n",
       "\n",
       "          Fecha                                  Responsabilidades  \\\n",
       "0  Hace 4 horas                            Liquidación de haberes.   \n",
       "1  Hace 9 horas  Asistencia, y valoración alteraciones del leng...   \n",
       "2  Hace 9 horas  Ajustarse a las normas comerciales del proveed...   \n",
       "3    Hace 1 día                                                  .   \n",
       "4    Hace 1 día                                                  .   \n",
       "\n",
       "                                           Funciones  \\\n",
       "0  Administrativo contable con conocimientos  y e...   \n",
       "1            Evaluación, asesoramiento, tratamientos   \n",
       "2  Ventas directas y Distribución independiente\\n...   \n",
       "3  Seleccionamos Ayudante de Panadería avanzado, ...   \n",
       "4  Seleccionamos limpiador o limpiadora para Pana...   \n",
       "\n",
       "                                          Requisitos  \n",
       "0  Experiencia de 1 año en el área de Recursos hu...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Descripción</th>\n      <th>Empresa</th>\n      <th>Cantidad de ofertas</th>\n      <th>Fecha</th>\n      <th>Responsabilidades</th>\n      <th>Funciones</th>\n      <th>Requisitos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ADMINISTRATIVO CONTABLE</td>\n      <td></td>\n      <td>1 ofertas laborales</td>\n      <td>Hace 4 horas</td>\n      <td>Liquidación de haberes.</td>\n      <td>Administrativo contable con conocimientos  y e...</td>\n      <td>Experiencia de 1 año en el área de Recursos hu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FONOAUDIÓLOGO/A</td>\n      <td>Consultorio privado</td>\n      <td>2 ofertas laborales</td>\n      <td>Hace 9 horas</td>\n      <td>Asistencia, y valoración alteraciones del leng...</td>\n      <td>Evaluación, asesoramiento, tratamientos</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VENDEDOR DISTRIBUIDOR FREELANCE</td>\n      <td>Diamante del Este</td>\n      <td>1 ofertas laborales</td>\n      <td>Hace 9 horas</td>\n      <td>Ajustarse a las normas comerciales del proveed...</td>\n      <td>Ventas directas y Distribución independiente\\n...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AYUDANTE DE PANADERÍA AVANZADO</td>\n      <td>Empresa</td>\n      <td>2 ofertas laborales</td>\n      <td>Hace 1 día</td>\n      <td>.</td>\n      <td>Seleccionamos Ayudante de Panadería avanzado, ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LIMPIADOR/A PARA PANADERIA</td>\n      <td>Empresa</td>\n      <td>2 ofertas laborales</td>\n      <td>Hace 1 día</td>\n      <td>.</td>\n      <td>Seleccionamos limpiador o limpiadora para Pana...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "puestos = []\n",
    "with requests.Session() as s:\n",
    "    for url in urls:\n",
    "        full_url = f\"https://trabajo.gallito.com.uy{url}\"\n",
    "        print(f\"Procesando {full_url}\")\n",
    "        r = s.get(full_url)\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        title = soup.find(\"div\", class_=\"title-puesto\").contents[0].text.strip()\n",
    "        subtitle = soup.find(\"div\", class_=\"subtitle-puesto\").text.strip()\n",
    "        since = soup.find(\"span\", class_=\"time-text\").text.strip()\n",
    "        cantidad = soup.find(\"div\", class_=\"span-ofertas\").text.strip()\n",
    "        titulo_textos = soup.find_all(\"div\", class_=\"cuadro-aviso-title\")\n",
    "        textos = soup.find_all(\"div\", class_=\"cuadro-aviso-text\")\n",
    "        textos = {k.text.strip(): v.text.strip() for k, v in zip(titulo_textos, textos)}\n",
    "        textos.pop(\"Avisos similares\", None)\n",
    "        data_puesto = {\"Descripción\": title, \"Empresa\": subtitle, \"Cantidad de ofertas\": cantidad, \"Fecha\": since}\n",
    "        data_puesto.update(textos)\n",
    "        puestos.append(data_puesto)\n",
    "\n",
    "df = pd.DataFrame(puestos)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "Algunos comentarios:\n",
    "\n",
    "1. Usamos una ``Session`` de ``requests`` para agilizar el proceso dado que estamos entrando a varias páginas del mismo sitio.\n",
    "2. Para el título tenemos que revisar los ``contents``, porque es un h2 dentro de un div. No podemos buscar directamente los h2 porque hay otros títulos de ese tipo en la página.\n",
    "3. El subtítulo (que tiene la información de la empresa), la cantidad de ofertas y el momento de publicación no tienen dificultades, simplemente accedemos a su ``text`` y le aplicamos el método ``strip()`` para que saque todos los espacios extra.\n",
    "4. Cada puesto tiene distintos boxes con información que no están presentes en todos los avisos. Sin embargo, siempre tienen los mismos tags. Por eso  hacemos un ``find_all()`` para esos tags y luego lo procesamos con una dict comprehension.\n",
    "5. Algunos avisos tienen un box de \"Avisos similares\" que no nos interesa por ahora, así que lo eliminamos del diccionario de textos.\n",
    "6. Construimos un nuevo diccionario con el título, subtítulo, cantidad de ofertas y momento de publicación.\n",
    "7. Actualizamos este diccionario con la información de los textos.\n",
    "8. Agregamos el diccionario completo a la lista de ``puestos``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}